"""
Tool definitions and handlers for Backboard LLM integration.
Tools are invoked via special prompts like @"toolname"
"""

import os
import re
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Any
from backboard import BackboardClient
import db


# Backboard tool definitions in JSON schema format
BACKBOARD_TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "create_file",
            "description": "Creates a file in the user's workspace. Generates appropriate content based on the filename and context.",
            "parameters": {
                "type": "object",
                "properties": {
                    "filename": {
                        "type": "string",
                        "description": "The file path where the file should be created (e.g., 'README.md', 'docs/ONBOARDING.md')"
                    }
                },
                "required": ["filename"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "get_recent_context",
            "description": "Retrieves recent activity from Telegram, Drive, and GitHub within the last X hours.",
            "parameters": {
                "type": "object",
                "properties": {
                    "hours": {
                        "type": "integer",
                        "description": "Number of hours to look back (default: 24)",
                        "default": 24
                    }
                },
                "required": []
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "generate_mermaid_graph",
            "description": "Generates a Mermaid flowchart showing the lineage of a topic/feature across Telegram discussions, Drive docs, and GitHub code.",
            "parameters": {
                "type": "object",
                "properties": {
                    "topic": {
                        "type": "string",
                        "description": "The topic or feature name to generate a graph for"
                    }
                },
                "required": ["topic"]
            }
        }
    }
]


def get_backboard_tools() -> List[Dict[str, Any]]:
    """Get all Backboard tool definitions."""
    return BACKBOARD_TOOLS


def detect_tool_invocation(content: str) -> Optional[str]:
    """
    Detect if the content contains a tool invocation pattern.
    Pattern: @"toolname" or @toolname
    
    Returns the tool name if found, None otherwise.
    """
    # Match patterns like @create_file
    pattern = r'@(\w+)'
    match = re.search(pattern, content)
    if match:
        return match.group(1)
    return None


async def handle_create_file(
    client_id: str,
    filename: str,
    backboard_client: BackboardClient,
    assistant_id: str,
    user_query: str
) -> Dict[str, Any]:
    """
    Tool: create_file
    Creates a new file in the user's workspace.
    Always generates content via Backboard based on filename and user query.
    """
    # Generate content via Backboard
    try:
        thread = await backboard_client.create_thread(assistant_id)
        
        generation_prompt = f"""Generate the complete content for a file at path: {filename}

Based on the filename and the user's request: "{user_query}"

Create appropriate, well-structured content for this file. The file should be production-ready and follow best practices for the file type.

Generate the complete file content now:"""
        
        output = []
        async for chunk in await backboard_client.add_message(
            thread_id=thread.thread_id,
            contbutent=generation_prompt,
            memory="auto",
            stream=True
        ):
            if chunk.get('type') == 'content_streaming':
                output.append(chunk.get('content', ''))
        
        content = "".join(output).strip()
        
        if not content:
            content = f"# {os.path.basename(filename)}\n\n*Content generated by Backboard*\n"
    except Exception as e:
        print(f"Error generating content with Backboard: {e}")
        content = f"# {os.path.basename(filename)}\n\n*Content generation failed, please edit this file*\n"
    
    return {
        "tool": "create_file",
        "status": "ready",
        "filename": filename,
        "content": content,
        "message": f"File creation request prepared: {filename}"
    }


async def handle_get_recent_context(
    client_id: str,
    backboard_client: BackboardClient,
    assistant_id: str,
    hours: int = 24
) -> Dict[str, Any]:
    """
    Tool: get_recent_context
    Retrieves RAG chunks ingested within the last X hours, grouped by source.
    
    This queries Backboard's RAG system for recent memories/chunks.
    """
    try:
        # Query Backboard for recent memories
        # Note: This is a simplified implementation - actual Backboard API may differ
        # We'll need to query memories with metadata filtering
        
        # Get all memories for the assistant

        try:
            memories = await backboard_client.get_memories(assistant_id=assistant_id)
        except AttributeError:
            memories = []
        
        # Filter by ingested_at timestamp (if available in metadata)
        now = datetime.now(timezone.utc)
        cutoff_time = now - timedelta(hours=hours)
        
        recent_context = {
            "telegram": [],
            "drive": [],
            "github": []
        }
        
        # Group memories by source
        for memory in memories:
            # Check if memory has metadata with source and ingested_at
            metadata = memory.get("metadata", {})
            source = metadata.get("source", "unknown")
            ingested_at_str = metadata.get("ingested_at")
            
            if ingested_at_str:
                try:
                    # Parse and normalize to UTC
                    ingested_at = datetime.fromisoformat(ingested_at_str.replace('Z', '+00:00'))
                    # Ensure timezone-aware (convert to UTC if naive)
                    if ingested_at.tzinfo is None:
                        ingested_at = ingested_at.replace(tzinfo=timezone.utc)
                    else:
                        ingested_at = ingested_at.astimezone(timezone.utc)
                    
                    if ingested_at >= cutoff_time:
                        memory_entry = {
                            "content": memory.get("content", "")[:200] + "...",  # Preview
                            "ingested_at": ingested_at_str,
                            "memory_id": memory.get("memory_id")
                        }
                        
                        if source.lower() == "telegram":
                            recent_context["telegram"].append(memory_entry)
                        elif source.lower() == "drive":
                            recent_context["drive"].append(memory_entry)
                        elif source.lower() in ["github", "git"]:
                            recent_context["github"].append(memory_entry)
                except (ValueError, AttributeError):
                    # Skip if timestamp parsing fails
                    pass
        
        # Format the response for the LLM
        formatted_response = format_recent_context(recent_context, hours)
        
        return {
            "tool": "get_recent_context",
            "status": "success",
            "hours": hours,
            "context": recent_context,
            "formatted": formatted_response
        }
    except Exception as e:
        return {
            "tool": "get_recent_context",
            "status": "error",
            "error": str(e)
        }


def escape_mermaid_string(text: str, max_length: int = 50) -> str:
    """
    Escape a string for safe use in Mermaid node labels.
    Escapes quotes, brackets, pipes, and newlines.
    """
    if not text:
        return ""
    
    # Truncate if too long
    if len(text) > max_length:
        text = text[:max_length] + "..."
    
    # Escape special Mermaid characters
    # Replace newlines with literal \n
    text = text.replace("\n", "\\n")
    # Escape quotes
    text = text.replace('"', '\\"')
    # Escape brackets (though brackets are used for node labels, we escape them in content)
    text = text.replace("[", "\\[")
    text = text.replace("]", "\\]")
    # Escape pipes (used in some Mermaid syntax)
    text = text.replace("|", "\\|")
    
    return text


def format_recent_context(context: Dict[str, List], hours: int) -> str:
    """
    Format recent context into a readable summary grouped by source.
    """
    output = f"ðŸ“Š Recent Activity (Last {hours} hours)\n\n"
    
    telegram_count = len(context["telegram"])
    drive_count = len(context["drive"])
    github_count = len(context["github"])
    
    if telegram_count > 0:
        output += f"ðŸ“¢ Chat Decisions (Source: Telegram) - {telegram_count} items\n"
        for item in context["telegram"][:5]:  # Show top 5
            output += f"  â€¢ {item['content']}\n"
        output += "\n"
    
    if drive_count > 0:
        output += f"ðŸ“„ New Specs (Source: Drive) - {drive_count} items\n"
        for item in context["drive"][:5]:
            output += f"  â€¢ {item['content']}\n"
        output += "\n"
    
    if github_count > 0:
        output += f"ðŸ’» Code Changes (Source: Github) - {github_count} items\n"
        for item in context["github"][:5]:
            output += f"  â€¢ {item['content']}\n"
        output += "\n"
    
    if telegram_count == 0 and drive_count == 0 and github_count == 0:
        output += "No recent activity found in the specified time period.\n"
    
    return output


async def handle_generate_mermaid_graph(
    client_id: str,
    topic: str,
    backboard_client: BackboardClient,
    assistant_id: str
) -> Dict[str, Any]:
    """
    Tool: generate_mermaid_graph
    Generates a Mermaid.js syntax string that maps the lineage of a feature.
    
    This queries Backboard for related memories about the topic and constructs
    a graph showing the relationship between chat decisions, specs, and code.
    """
    try:
        # Query Backboard for memories related to the topic
        try:
            memories = await backboard_client.get_memories(assistant_id=assistant_id)
        except AttributeError:
            # Fallback if get_memories doesn't exist
            memories = []
        
        # Filter memories related to the topic
        related_memories = []
        for memory in memories:
            content = memory.get("content", "").lower()
            if topic.lower() in content:
                related_memories.append(memory)
        
        # Build Mermaid graph
        mermaid_syntax = build_mermaid_graph(topic, related_memories)
        
        return {
            "tool": "generate_mermaid_graph",
            "status": "success",
            "topic": topic,
            "mermaid": mermaid_syntax,
            "formatted": f"```mermaid\n{mermaid_syntax}\n```"
        }
    except Exception as e:
        return {
            "tool": "generate_mermaid_graph",
            "status": "error",
            "error": str(e)
        }


def build_mermaid_graph(topic: str, memories: List[Dict]) -> str:
    """
    Build a Mermaid flowchart showing the lineage of a feature.
    """
    # Escape topic for safe use in Mermaid
    escaped_topic = escape_mermaid_string(topic, max_length=100)
    
    graph = f"graph TD\n"
    graph += f"    Start[\"{escaped_topic}\"]\n"
    
    # Group memories by source
    telegram_nodes = []
    drive_nodes = []
    code_nodes = []
    
    for i, memory in enumerate(memories[:10]):  # Limit to 10 for readability
        metadata = memory.get("metadata", {})
        source = metadata.get("source", "unknown").lower()
        content = memory.get("content", "")
        # Escape content before truncating
        escaped_content = escape_mermaid_string(content, max_length=50)
        node_id = f"Node{i}"
        
        if source == "telegram":
            telegram_nodes.append((node_id, escaped_content))
        elif source == "drive":
            drive_nodes.append((node_id, escaped_content))
        elif source in ["github", "git"]:
            code_nodes.append((node_id, escaped_content))
    
    # Add nodes and edges
    if telegram_nodes:
        graph += f"    Start --> Telegram[\"Telegram Discussions\"]\n"
        for node_id, content in telegram_nodes:
            graph += f"    Telegram --> {node_id}[\"{content}\"]\n"
    
    if drive_nodes:
        graph += f"    Start --> Drive[\"Drive Documents\"]\n"
        for node_id, content in drive_nodes:
            graph += f"    Drive --> {node_id}[\"{content}\"]\n"
    
    if code_nodes:
        graph += f"    Start --> Code[\"Code Implementation\"]\n"
        for node_id, content in code_nodes:
            graph += f"    Code --> {node_id}[\"{content}\"]\n"
    
    if not telegram_nodes and not drive_nodes and not code_nodes:
        graph += f"    Start --> NoData[\"No related data found\"]\n"
    
    return graph


async def execute_tool(
    tool_name: str,
    client_id: str,
    content: str,
    backboard_client: BackboardClient,
    assistant_id: str
) -> Dict[str, Any]:
    """
    Execute a tool based on its name.
    
    Args:
        tool_name: Name of the tool to execute
        client_id: Client ID
        content: Original user content (may contain tool parameters)
        backboard_client: Backboard client instance
        assistant_id: Assistant ID
    
    Returns:
        Tool execution result
    """
    # Parse tool parameters from content
    # This is a simplified parser - in production, you'd want more robust parsing
    
    if tool_name == "create_file":
        # Extract filename from natural language query
        # Look for file patterns like "README.md", "docs/file.txt", etc.
        filename_match = re.search(r'\b([a-zA-Z0-9_\-./]+\.(md|txt|py|js|ts|json|yaml|yml|xml|html|css|sh|bat|yml))\b', content, re.IGNORECASE)
        if filename_match:
            filename = filename_match.group(1)
        else:
            # Try to extract from phrases like "create a file called X" or "file named X"
            named_match = re.search(r'(?:file|file called|file named|create)\s+["\']?([a-zA-Z0-9_\-./]+)["\']?', content, re.IGNORECASE)
            if named_match:
                filename = named_match.group(1)
            else:
                # Default fallback
                filename = "docs/ONBOARDING.md"
        
        # Always generate content via Backboard
        return await handle_create_file(
            client_id, filename, backboard_client, assistant_id, user_query=content
        )
    
    elif tool_name == "get_recent_context":
        # Extract hours from natural language (e.g., "last 48 hours", "past 2 hours")
        hours_match = re.search(r'(?:last|past|within)\s+(\d+)\s+hours?', content, re.IGNORECASE)
        if hours_match:
            hours = int(hours_match.group(1))
        else:
            # Try direct number
            num_match = re.search(r'(\d+)\s*hours?', content, re.IGNORECASE)
            hours = int(num_match.group(1)) if num_match else 24
        
        return await handle_get_recent_context(
            client_id, backboard_client, assistant_id, hours
        )
    
    elif tool_name == "generate_mermaid_graph":
        # Extract topic from natural language (remove tool name and common words)
        topic = content.replace(f"@{tool_name}", "").replace("generate", "").replace("mermaid", "").replace("graph", "").strip()
        # Clean up: remove "for", "of", "about" and take the rest
        topic = re.sub(r'^(?:for|of|about|show|create)\s+', '', topic, flags=re.IGNORECASE).strip()
        if not topic:
            topic = "feature lineage"
        
        return await handle_generate_mermaid_graph(
            client_id, topic, backboard_client, assistant_id
        )
    
    else:
        return {
            "tool": tool_name,
            "status": "error",
            "error": f"Unknown tool: {tool_name}"
        }

